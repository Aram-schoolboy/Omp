| Лабораторная работа №4 | M3102                  | АОВС |
| ---------------------- |------------------------| ---- |
| OpenMP                 | Мурадян Арам Суренович | 2024 |


## Инструментарий
Инструментарий - g++ 13.1.0

## Результат работы на тестовых данных: https://github.com/skkv-itmo-comp-arch/se-comp-arch24-omp-Aram-schoolboy/actions/runs/9341136671

# Описание:

## Результат работы.
Процессор - 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz 2.30 GHz.  
Лучшие показатели наблюдались при 16 потоках и static schedule без указания chunk size.  
Среднее значение при N = 10^8 составило 1035 ms.  
Среднее значение ответа составило 1.2566

## Метод Монте-Карло.
Метод Монте-Карло заключается в том, что мы генерируем N случайных точек в пространстве. Каждая точка либо попадает в фигуру, объем которой мы хотим посчитать, либо не попадает. Тогда объем фигуры будет рассчитан по формуле (попавшие точки / N) * объем фигуры, внутри которой полностью лежит заданная фигура.  
Причём чем меньше фигура, внутри которой лежит заданная, тем лучше точность. Поэтому выбранная фигура - параллелепипед - имеет минимальные и максимальные координаты заданного периформа - они были посчитаны с помощью производной. Уравнение поверхности периформа выглядит так: (x^4-ax^3)+a^2(y^2+z^2)=0. Если точка попадает внутрь периформа, то соответствующее значение функции меньше нуля. Если больше - то вне фигуры. В случае равенства точка лежит на поверхности фигуры.


## Генерация случайных вещественных чисел.
Чтобы получить вещественное число из диапазона [a; b], давайте сначала научимся генерировать случайные числа от 0 до до 2^32 - 1.  
Далее это случайное число p разделим на 2^32 - 1. Получим какое-то вещественное число в диапазоне [0; 1]. Тогда чтобы получить число t из диапазона [a; b], необходимо воспользоваться следующей формулой: t = a + (b - a) * p.  
Понятно, что b - a > 0 и (b - a) * p <= (b - a). А значит a <= a + (b - a) * p <= b, что и требовалось.

### Генерация случайного целого неотрицательного числа от 0 до 2^32 - 1.
Основной решения данной задачи стал std::mt19937. Этот генератор строится на Вихре Мерсенна - алгоритме генерации псевдослучайных чисел, который использует простые числа Мерсенна - некоторое простое число Мерсенна становится периодом генерации. Генератор выдаёт числа от 0 до 2^32 - 1. Задавать сид - стартовое число будем с помощью случайной формулы, которая так же будет использовать и номер потока, чтобы оно было у всех потоков разным. Генератор std::mt19937_64 хоть и немного выигрывал по точности на небольших N, но производительность была значительно ниже. Rand() обладает двумя серьезными минусами. Во-первых, это числа низкого качества - точность от этого сильно страдает. Во-вторых, стартовое число задаётся с помощью srand() и задать каждому потоку свой srand() не представляется возможным, ведь srand() задаёт некоторое глобальное состояние.

## Описание конструкций OpenMP для распараллеливания команд, которые использовались и как они работают.
### omp_set_num_threads(int) - эта функция позволяет установить произвольное количество потоков.
### omp_get_num_threads() - эта функция возвращает количество потоков.

### #pragma omp parallel if(condition) shared(var1, var2, ..., varn)
Данная запись позволяет объявить параллельный участок кода. Будет создано OMP_NUM_THREADS потоков. Именно эту переменную мы меняем и запрашиваем функциями, описанными выше.  
if(condition) позволяет коду выполниться последовательно и не создавать потоки, если condition = False.  
shared(v1, v2, ..., varn) делает переменные v1, v2, ..., vn shared. Переменная может быть private либо shared. Private переменная принадлежит только одному потоку, в то время как доступ к shared переменной имеют все потоки из team threads.

### #pragma omp single
code;  
Директива omp single позволяет отдать на выполнение следующий код только какому-то одному потоку. Т.е. следующий код не будет выполнен всеми потоками.

### #pragma omp critical
code;
Директива omp critical выстраивает все потоки в очередь и позволяет "просуммировать" общий итог работы потоков. На конкретном примере это будет более ясно, однако общий смысл в том, что не происходит ошибки при запросе доступа к общей переменной, ведь они обращаются к ней по порядку.


### #pragma omp for schedule(kind, chunk_size)
#### schedule(static, chunk_size) на этапе комплияции понимает, на сколько блоков размера chunk_size поделить общее количество итераций цикла. Далее эти блоки равномерно распределяются по потокам.

#### schedule(dynamic, chunk_size) динамически распределяет блоки размера chunk_size. До цикла каждый поток получает по одному блоку. Далее, новую "партию" блоков они получат только тогда, когда каждый поток выполнит свой блок.

## Описание работы написанного кода. Как реализовано распараллеливание кода при помощи ранее описанных конструкций.
Основная функция CalculateAnswer(args) содержит в себе цикл, генерирующий точки, записывает кол-во потоков и записывает ответ.  
Из-за наличия --no-omp, необходимо в if(flag) передать flag, чтобы исключить распараллеливание без должной необходимости.  
В цикле для генерации точек используются минимальные координаты параллелепипеда и модули разности координат. Они не изменяются и нужны только для подсчета в формуле. Поэтому логично сделать эти переменные shared.  
Количество потоков записывается в глобальную переменную threads. Если это будут делать все потоки, то поведение может оказаться непредсказуемым, т.к. все потоки будут пытаться записать в одну и ту же область памяти некоторое значение. Поэтому избежим этого с помощью omp single. Теперь только 1 поток запишет в переменную нужное значение.  
Используем omp for schedule(static) для распараллеливания основного цикла. Static без указания chunk_size показал наилучшие результаты. Так же создаём переменные size_t hit, Generator generator. У каждого потока они будут свои, т.е. private.  
После работы цикла, необходимо просуммировать все значения hit. Для этого заведем глоабльную переменную total_hit = 0 и с помощью omp critical прибавим к total_hit значение hit каждого потока.  
После всего этого посчитаем ответ по формуле - в потоках для этого необходимости нет, поэтому эта строчка кода вне параллельного кода.

## Тестирование. 
### NO OMP and one thread.
![NO OMP and one thread](https://github.com/skkv-itmo-comp-arch/se-comp-arch24-omp-Aram-schoolboy/assets/52756403/f626b206-6aa6-4b2b-9471-13216c07db5c)
С параметром NO_OMP время работы совпадает с schedule(static) и одним потоком. Понятно, что при параметре static без указания chunk_size и единственным потоком, вся нагрузка придётся на единственный поток, как это бы и происходило при NO_OMP. Отсюда и одинаковое время работы. NO_OMP работает чуть быстрее, т.к. исключаются бесполезные накладные расходы.  
schedule(dynamic) работает значительно медленнее. Такая конфигурация для 1 потока не просто бесполезна, но и сильно замедляет работу. Очевидно, что весь объем итераций должен будет сделать единственный поток, однако из-за schedule(dynamic) этот объем будет подаваться постепенно, из-за чего будут возникать задержки и накладные расходы, а как результат получается сильное ухудшение производительности.

### Static schedule. Dependence of time of threads.
![Static schedule  Dependence of time of threads](https://github.com/skkv-itmo-comp-arch/se-comp-arch24-omp-Aram-schoolboy/assets/52756403/d44bff8b-1131-476e-84df-1a44bd134977)
Когда мы имеем один поток, вся нагрузка приходится на него одного. Когда их двое, нагрузка распределяется примерно поровно, что двукратно увеличивает скорость работы. Добавляя третий поток скорость увеличивается еще в ~1.5 раза. Понятно, что с каждым новым потоком скорость улучшается всё меньше и меньше. К тому же, в какой-то момент ядра заканчиваются и прироста никакого наблюдаться не будет. И если переборщить, наоборот, будет наблюдаться ухудшение производительности, т.к. необоснованно большое количество потоков будет потреблять много ресурсов, но пользы от такой кучи потоков будет мало.

### Static schedule. 16 threads. Chunk size dependence.
![Static schedule  16 threads  Chunk size dependence](https://github.com/skkv-itmo-comp-arch/se-comp-arch24-omp-Aram-schoolboy/assets/52756403/94d78d75-a9a3-4328-bf38-463443e26927)
График весьма и весьма стабилен. Напрашивающийся вывод - скорость работы schedule(static) практически не зависит от chunk_size. Это логично, потому что всё, что необходимо сделать библиотеке open-mp - разбить итерации цикла на блоки размера chunk_size и затем распределить их по потокам. Это необходимо сделать всего лишь один раз. Поэтому данная операция не может заниматься много времени. А далее каждый поток выполняет спокойно свой набор блоков.

### Dynamic schedule. Dependence of time of threads.
![Dynamic schedule  Dependence of time of threads](https://github.com/skkv-itmo-comp-arch/se-comp-arch24-omp-Aram-schoolboy/assets/52756403/9d461f44-4a57-4e92-ab3d-7a05c96fdbfd)
Чем больше потоков - тем дольше выполняется программа. В нашем случае все итерации цикла по нагрузке равноценны. На каждой итерации происходит одно и то же - генерируются случайные числа, а далее проводятся арифметические операции. Разве что числа могут быть то большие, то маленькие и от этого скорость вычисления будет меняться, но это незначительный нюанс. Я считаю, что в нашем случае dynamic - бессмысленный параметр, потому что, как я уже отметил, нагрузка при каждой итерации равноценная. А это значит, что нет смысла динамически распределять блоки по потокам по мере их выполнения. Отвечая на вопрос, почему растёт время с увеличением кол-ва потоков, отмечу, что чем больше потоков, тем выше шанс, что какие-то потоки будут простаивать без дела, дожидаясь завершения обработки блоков другими потоками. Кроме того, чем больше потоков, тем больше раз необходимо будет выдавать какому-то потоку блок. И в дополнение к этому, опять же стоит отметить накладные расходы - необходимо следить, когда все потоки обработают свои блоки, заново выдать всем по одному блоку и т.д.

### Dynamic schedule. 16 threads. Chunk size dependence.
![Dynamic schedule  16 threads  Chunk size dependence](https://github.com/skkv-itmo-comp-arch/se-comp-arch24-omp-Aram-schoolboy/assets/52756403/0faea49c-9ea2-496d-b02a-85381b18058f)
С увеличением chunk_size время работы значительно снизилось и даже приблизилось ко времени работы schedule(static) на 16 потоках. Чем больше chunk_size, тем меньше раз придётся выдавать новые блоки потокам. А из предыдущего вывода, утверждающего, что именно это и тормозит работу, производительность должна кратно увеличиться - именно это мы и наблюдаем. Увеличив chunk_size с 1 до 60, т.е. в 60 раз, скорость увеличилась в 30 раз.

### Best configuration (static schedule). Dependence of time of threads.
![Best configuration (static schedule)  Dependence of time of threads](https://github.com/skkv-itmo-comp-arch/se-comp-arch24-omp-Aram-schoolboy/assets/52756403/aed8a313-acc6-435f-9cd0-a8c9df6a1de0)
Лучшие показатели имеет schedule(static). Выставлять свой chunk_size смысла мало, учитывая соответствующий график. К тому же без явного указания chunk_size программа показывает наилучший по скорости результат. Dynamic же проигрывает во всём. Хоть ему и удалось приблизиться к static при большом chunk_size, однако он всё равно уступает static.

